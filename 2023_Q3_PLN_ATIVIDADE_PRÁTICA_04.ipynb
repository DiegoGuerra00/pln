{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DiegoGuerra00/pln/blob/main/2023_Q3_PLN_ATIVIDADE_PR%C3%81TICA_04.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y6QILOdpOjwv"
      },
      "source": [
        "# **Processamento de Linguagem Natural [2023.Q3]**\n",
        "Prof. Alexandre Donizeti Alves"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8m67OOx9MX_3"
      },
      "source": [
        "### **ATIVIDADE PRÁTICA 04 [Uso da API da OpenAI com técnicas de PLN]**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Gk0nHKabBT-"
      },
      "source": [
        "A **ATIVIDADE PRÁTICA 04** deve ser feita utilizando o **Google Colab** com uma conta sua vinculada ao Gmail. O link do seu notebook, armazenado no Google Drive, além do link de um repositório no GitHub e os principais resultados da atividade, devem ser enviados usando o seguinte formulário:\n",
        "\n",
        "> https://forms.gle/GzwCq3R7ExtE9g9a8\n",
        "\n",
        "\n",
        "**IMPORTANTE**: A submissão deve ser feita até o dia **26/11 (domingo)** APENAS POR UM INTEGRANTE DA EQUIPE, até às 23h59. Por favor, lembre-se de dar permissão de ACESSO IRRESTRITO para o professor da disciplina de PLN."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D7hJlilKM485"
      },
      "source": [
        "### **EQUIPE**\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**POR FAVOR, PREENCHER OS INTEGRANDES DA SUA EQUIPE:**\n",
        "\n",
        "\n",
        "**Integrante 01:**\n",
        "\n",
        "`Por favor, informe o seu nome completo e RA:` Diego Guerra / RA:11201810534\n",
        "\n",
        "**Integrante 02:**\n",
        "\n",
        "`Por favor, informe o seu nome completo e RA:` Isolda Costa / RA: 21053014\n",
        "\n",
        "**Integrante 03:**\n",
        "\n",
        "`Por favor, informe o seu nome completo e RA:` Luis Gustavo Campos / RA: 11201811265"
      ],
      "metadata": {
        "id": "tnIArN0QY-Ek"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **LIVRO**\n",
        "---"
      ],
      "metadata": {
        "id": "6yExhaebs-nD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`Processamento de Linguagem Natural - Conceitos, Técnicas e Aplicações em Português.`\n",
        "\n",
        ">\n",
        "\n",
        "Disponível gratuitamente em:\n",
        "  \n",
        "  > https://brasileiraspln.com/livro-pln/1a-edicao/.\n",
        "\n",
        "\n",
        "**POR FAVOR, PREENCHER OS CAPITULOS SELECIONADOS PARA A SUA EQUIPE:**\n",
        "\n",
        "`Primeiro capítulo: ` 3\n",
        "\n",
        "`Segundo capítulo:` 23\n",
        "\n"
      ],
      "metadata": {
        "id": "DjJM_qhEZRy6"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EtjgWQRzNphL"
      },
      "source": [
        "### **DESCRIÇÃO**\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implementar um `notebook` no `Google Colab` que faça uso da **API da OpenAI** aplicando, no mínimo, 3 técnicas de PLN. As técnicas devem ser aplicadas nos 2 (DOIS) capítulos do livro **Processamento de Linguagem Natural - Conceitos, Técnicas e Aplicações em Português**.\n",
        "\n",
        ">\n",
        "\n",
        "**RESTRIÇÃO**: É obrigatório usar o *endpoint* \"*`Chat Completions`*\".\n",
        "\n",
        ">\n",
        "\n",
        "As seguintes técnicas de PLN podem ser usadas:\n",
        "\n",
        "*   Correção Gramatical\n",
        "*   Classificação de Textos\n",
        "*   Análise de Sentimentos\n",
        "*   Detecção de Emoções\n",
        "*   Extração de Palavras-chave\n",
        "*   Tradução de Textos\n",
        "*   Sumarização de Textos\n",
        "*   **Similaridade de Textos**\n",
        "*   **Reconhecimento de Entidades Nomeadas**\n",
        "*   **Sistemas de Perguntas e Respostas**\n",
        "\n",
        ">\n",
        "\n",
        "Os capítulos devem ser os mesmos selecionados na **ATIVIDADE PRÁTICA 02**. Para consultar os capítulos, considere a seguinte planilha:\n",
        "\n",
        ">\n",
        "\n",
        "> https://docs.google.com/spreadsheets/d/1ZutzQ3v1OJgsgzCvCwxXlRIQ3ChXNlHNvB63JQvYsbo/edit?usp=sharing\n",
        "\n",
        ">\n",
        ">\n",
        "\n",
        "**IMPORTANTE:** É obrigatório usar o e-mail da UFABC. Não é permitido alterar os capítulos já selecionados.\n",
        "\n"
      ],
      "metadata": {
        "id": "fXTwkiiGs2BV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **CRITÉRIOS DE AVALIAÇÃO**\n",
        "---\n"
      ],
      "metadata": {
        "id": "gWsBYQNtxmum"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Serão considerados como critérios de avaliação as técnicas usadas e a criatividade envolvida na aplicação das mesmas.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "5iHdx4BXYruQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **IMPLEMENTAÇÃO**\n",
        "---"
      ],
      "metadata": {
        "id": "nw09lujGvfjc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Configuração da API"
      ],
      "metadata": {
        "id": "rM3yeV3tKQPV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai==0.28.1"
      ],
      "metadata": {
        "id": "RyUailD5vi9E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fef9c700-4e4a-4d6a-c7ff-006c907dead0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai==0.28.1\n",
            "  Downloading openai-0.28.1-py3-none-any.whl (76 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.0/77.0 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai==0.28.1) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai==0.28.1) (4.66.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai==0.28.1) (3.8.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28.1) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28.1) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28.1) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28.1) (2023.7.22)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28.1) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28.1) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28.1) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28.1) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28.1) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28.1) (1.3.1)\n",
            "Installing collected packages: openai\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "llmx 0.0.15a0 requires tiktoken, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed openai-0.28.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from getpass import getpass\n",
        "\n",
        "OPENAI_API_KEY = getpass()"
      ],
      "metadata": {
        "id": "jSDo3JAsf0uP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a4de429-fcae-4d68-bcc2-00f5a4e8d95e"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "\n",
        "openai.api_key = OPENAI_API_KEY"
      ],
      "metadata": {
        "id": "tPHAJeuDgk_c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Obtendo o texto dos capítulos"
      ],
      "metadata": {
        "id": "b1iRzouiLl1V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "url_cap_3 = 'https://brasileiraspln.com/livro-pln/1a-edicao/parte2/cap3/cap3.html'\n",
        "url_cap_23 = 'https://brasileiraspln.com/livro-pln/1a-edicao/parte9/cap23/cap23.html'\n",
        "\n",
        "response_cap_3 = requests.get(url_cap_3)\n",
        "response_cap_23 = requests.get(url_cap_23)"
      ],
      "metadata": {
        "id": "VzfBD3hTEvGg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup\n",
        "from random import randint\n",
        "\n",
        "# Trabalharemos com paragrafos aleatórios dos capítulos\n",
        "# para evitar exceder os limites da API\n",
        "def get_text_from_paragraph(response):\n",
        "  soup = BeautifulSoup(response.content, 'html.parser')\n",
        "  paragraphs = soup.find_all('p')\n",
        "\n",
        "  return paragraphs[randint(0, len(paragraphs))].get_text()\n",
        "\n",
        "def get_all_text(response):\n",
        "  soup = BeautifulSoup(response.content, 'html.parser')\n",
        "  paragraphs = soup.find_all('p')\n",
        "\n",
        "  text = ''\n",
        "  for p in paragraphs:\n",
        "    text += p.get_text()\n",
        "\n",
        "  print(text)\n",
        "  return text"
      ],
      "metadata": {
        "id": "QzzvFLyLGvgU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Decidimos trabalhar com parágrafos aleatórios dos capitulos para evitar exceder\n",
        "os limites de uso da API da OpenAI**"
      ],
      "metadata": {
        "id": "XU-YILK3A6C9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Correções Gramaticais"
      ],
      "metadata": {
        "id": "CQoMeUOMKH3G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Recebe um parágrafo em pt_br e corrige erros gramaticais\n",
        "def correct_text(text):\n",
        "  instruction = \"Você irá receber um texto em Português do Brasil e irá corrigir qualquer erro gramatical encontrato, retornando a frase 'Texto Corrigido: {texto}', onde {texto} deve ser o texto com os erros corrigidos. Após isto, caso algum erro tenha sido encontrado, retorne todos os erros corrigidos no formato '{erro gramatical} -> {erro corrigido}' e nada mais. Caso nenhum erro seja encontrado não retorne nada\"\n",
        "\n",
        "  print(\"Texto original: {}\".format(text))\n",
        "  ai_response = openai.ChatCompletion.create(\n",
        "      model = 'gpt-3.5-turbo',\n",
        "      messages = [\n",
        "          {\"role\": \"system\", \"content\": instruction},\n",
        "          {\"role\": \"user\", \"content\": text}\n",
        "      ]\n",
        "  )\n",
        "\n",
        "  print(ai_response[\"choices\"][0][\"message\"][\"content\"])"
      ],
      "metadata": {
        "id": "-xVoLjynxt7a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Uso da técnica nos capitulos"
      ],
      "metadata": {
        "id": "rWi3KI3VPUP1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Capítulo 3:\")\n",
        "correct_text(get_text_from_paragraph(response_cap_3))"
      ],
      "metadata": {
        "id": "WSgwrIVBPYlE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "048d8c27-62c5-4d59-9757-08ac964b8315"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Capítulo 3:\n",
            "Texto original: Uma das etapas cruciais desse projeto foi a preparação do corpus CORAA-SER, que consiste em aproximadamente 1 hora de áudio de fala espontânea, anotado com presença ou ausência de emoção, envolvendo homens e mulheres. O corpus foi obtido a partir de anotações paralinguísticas de outro corpus denominado C-ORAL–BRASIL I, um corpus de referência do português brasileiro falado informal (Raso; Mello, 2012b). A primeira versão do CORAA-SER está disponível publicamente28. Com o CORAA-SER, já foi possível explorar diferentes técnicas de representação e métodos de aprendizado de máquina para identificar padrões emocionais na fala espontânea em português. Uma visão geral com os resultados de diferentes trabalhos e grupos de pesquisa foram sumarizados por Marcacini; Candido Junior; Casanova (2022).\n",
            "Texto Corrigido: Uma das etapas cruciais desse projeto foi a preparação do corpus CORAA-SER, que consiste em aproximadamente 1 hora de áudio de fala espontânea, anotado com a presença ou ausência de emoção, envolvendo homens e mulheres. O corpus foi obtido a partir de anotações paralinguísticas de outro corpus denominado C-ORAL-BRASIL I, um corpus de referência do português brasileiro falado informal (Raso; Mello, 2012b). A primeira versão do CORAA-SER está disponível publicamente28. Com o CORAA-SER, já foi possível explorar diferentes técnicas de representação e métodos de aprendizado de máquina para identificar padrões emocionais na fala espontânea em português. Uma visão geral com os resultados de diferentes trabalhos e grupos de pesquisa foi sumarizada por Marcacini; Candido Junior; Casanova (2022).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nCapítulo 23:\")\n",
        "correct_text(get_text_from_paragraph(response_cap_23))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "suIsjUB9AcKj",
        "outputId": "d9b32aa9-e7c4-4ce8-bb3d-1c5c2132c93c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Capítulo 23:\n",
            "Texto original: O ToLD-Br (Leite et al., 2020) é um conjunto de dados capturado do Twitter/X entre julho e agosto de 2019 com a ferramenta GATE Cloud’s Twitter Collector12. Elaborado para estudos sobre classificação automática de comentários tóxicos, este conjunto de dados tem como objetivo equilibrar o viés de anotação. Para tanto, 42 anotadores foram selecionados, com base em suas informações demográficas. Este corpus apresenta um conjunto de 21000 tweets em português manualmente anotados por três diferentes anotadores em sete categorias: LGBTQ+fobia, obsceno, insulto, racismo, misoginia e/ou xenofobia.\n",
            "Texto Corrigido: O ToLD-Br (Leite et al., 2020) é um conjunto de dados capturado do Twitter/X entre julho e agosto de 2019 com a ferramenta GATE Cloud’s Twitter Collector. Elaborado para estudos sobre classificação automática de comentários tóxicos, este conjunto de dados tem como objetivo equilibrar o viés de anotação. Para tanto, 42 anotadores foram selecionados, com base em suas informações demográficas. Este corpus apresenta um conjunto de 21.000 tweets em português manualmente anotados por três diferentes anotadores em sete categorias: LGBTQ+fobia, obsceno, insulto, racismo, misoginia e/ou xenofobia.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Reconhecimento de Entidades Nomeadas"
      ],
      "metadata": {
        "id": "hN-a0F0uPq3f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ren(text):\n",
        "  instruction = \"Você irá receber um texto em Português do Brasil e irá realizar o Reconhecimento de Entidades Nomeadas, retornando apenas as Entidades encontradas no formato de uma lista do Python e nada mais\"\n",
        "\n",
        "  print(\"Texto: {}\".format(text))\n",
        "  ai_response = openai.ChatCompletion.create(\n",
        "      model = 'gpt-3.5-turbo',\n",
        "      messages = [\n",
        "          {\"role\": \"system\", \"content\": instruction},\n",
        "          {\"role\": \"user\", \"content\": text}\n",
        "      ]\n",
        "  )\n",
        "\n",
        "  print(ai_response[\"choices\"][0][\"message\"][\"content\"])\n"
      ],
      "metadata": {
        "id": "z2RPR3Ipx57H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Uso da Técnica nos Capítulos"
      ],
      "metadata": {
        "id": "Q9j24nha1_ZV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Capítulo 3:\")\n",
        "ren(get_text_from_paragraph(response_cap_3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oYzK2eCY0tG6",
        "outputId": "fe0fff50-3e3e-4e93-cc5d-1fd2b97ad361"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Capítulo 3:\n",
            "Texto: R – Ela veio para São Paulo no final dos anos 40, porque a situação em Pernambuco estava no campo, estava muito difícil, havia seca, então ela e as duas irmãs e o irmão vieram para São Paulo, início para trabalhar na casa de uma tia dela, no bairro da Penha, que tinha uma pensão. Aí depois cada um foi, como a maioria dos nordestinos, chega, fica na casa dos familiares, depois vai arrumando emprego, aí vai arrumando sua vida. Ou seja, minha mãe e meu pai também vieram para São Paulo, porque lá em Minas não havia trabalho. E ele, como tinha essa vontade de trabalhar, acredito eu, antes de completar 18 anos ele fugiu de casa, veio para São Paulo. Ele era o caçula do primeiro casamento da minha avó.\n",
            "['São Paulo', 'Pernambuco', 'Penha', 'Minas']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nCapítulo 23:\")\n",
        "ren(get_text_from_paragraph(response_cap_23))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HM1f4TtvAaBD",
        "outputId": "8f4756d2-fdc7-4b1f-b094-30026ed5375e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Capítulo 23:\n",
            "Texto: O Whatsapp8 permite que os usuários troquem mensagens privadas. Apesar de ser usado principalmente para conversas individuais, o WhatsApp possui recursos de grupos de conversação, onde podem participar até 256 usuários, e encaminhamento de mensagens (Cabral et al., 2021). Concebido como um aplicativo de mensagens instantâneas, o WhatsApp evoluiu para uma plataforma multifacetada, permitindo não apenas conversas privadas, mas também a formação de grupos e comunidades, compartilhamento de mídia, chamadas de voz e vídeo e até mesmo recursos empresariais.\n",
            "['Whatsapp', 'WhatsApp', 'Cabral et al.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Extração de palavras-chave"
      ],
      "metadata": {
        "id": "F24fKqFzPs38"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def key_words(text):\n",
        "  instruction = \"Você irá receber um texto em Português do Brasil e irá extrair todas as palavras-chave encontradas nele, retornando-as no formato de uma lista do Python e nada mais.\"\n",
        "\n",
        "  print(\"Texto: {}\".format(text))\n",
        "  ai_response = openai.ChatCompletion.create(\n",
        "      model = 'gpt-3.5-turbo',\n",
        "      messages = [\n",
        "          {\"role\": \"system\", \"content\": instruction},\n",
        "          {\"role\": \"user\", \"content\": text}\n",
        "      ]\n",
        "  )\n",
        "\n",
        "  print(ai_response[\"choices\"][0][\"message\"][\"content\"])"
      ],
      "metadata": {
        "id": "27h68104-nBK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Uso da Técnica nos Capítulos"
      ],
      "metadata": {
        "id": "T_tzSZVsET8I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Capítulo 3:\")\n",
        "key_words(get_text_from_paragraph(response_cap_3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YI4L4H-b_8iU",
        "outputId": "6affa170-35bc-4e4c-ac61-f6615dce6010"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Capítulo 3:\n",
            "Texto: Para o português brasileiro, trabalhos desenvolvidos no âmbito do projeto C-ORAL–Brasil avançam os estudos para a detecção automática de fronteiras prosódicas na fala espontânea a partir de parâmetros fonético-acústicos e fronteiras identificadas perceptualmente por anotadores treinados (Raso; Teixeira; Barbosa, 2020; Teixeira, 2022; Teixeira; Mittman, 2018; Teixeira; Barbosa; Raso, 2018). Os estudos utilizam excertos de fala monológica masculina (8–24 minutos de áudio e 1339–3697 palavras), provenientes dos corpora anotados C-ORAL–Brasil I e II (Mello; Raso; Almeida Ferrari, no prelo; Raso; Mello, 2012a). No âmbito do projeto TaRSila, o CORAA NURC-SP, que vem sendo preparado tanto para viabilizar estudos linguísticos quanto o processamento computacional, contará com \\(\\approx\\)334 horas de fala transcrita, das quais pelo menos 40 horas serão prosodicamente anotadas.\n",
            "['português brasileiro', 'trabalhos desenvolvidos', 'âmbito', 'projeto C-ORAL–Brasil', 'avançam', 'estudos', 'detecção automática', 'fronteiras prosódicas', 'fala espontânea', 'parâmetros fonético-acústicos', 'fronteiras identificadas', 'perceptualmente', 'anotadores treinados', 'Raso', 'Teixeira', 'Barbosa', '2020', '2022', 'Mittman', '2018', 'fala monológica masculina', '8–24 minutos', 'áudio', '1339–3697 palavras', 'corpora anotados', 'C-ORAL–Brasil I', 'II', 'Mello', 'Almeida Ferrari', 'prelo', 'Raso', 'Mello', '2012a', 'projeto TaRSila', 'CORAA NURC-SP', 'preparado', 'viabilizar estudos linguísticos', 'processamento computacional', '334 horas', 'fala transcrita', 'pelo menos 40 horas', 'prosodicamente anotadas']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nCapítulo 23:\")\n",
        "key_words(get_text_from_paragraph(response_cap_23))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XmHv4roFAePz",
        "outputId": "f962eaff-fec9-4d39-9c51-98ed8ac8be18"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Capítulo 23:\n",
            "Texto: As redes sociais se popularizaram no Brasil em 2004, com a criação do Orkut1. Desde lá, novas redes surgiram e com elas a percepção da necessidade e viabilidade de aplicação de abordagens em PLN para o estudo de conteúdos e comportamentos gerados nesse meio. Dentre as áreas de aplicação dessas abordagens, destacam-se a detecção de discurso de ódio e linguagem ofensiva, a detecção de ironia/sarcasmo/humor, a detecção de notícias falsas, a análise de sentimento, entre outras (Ferreira et al., 2017).\n",
            "['redes sociais', 'popularizaram', 'Brasil', '2004', 'criação', 'Orkut1.', 'novas redes', 'surgiram', 'percepção', 'necessidade', 'viabilidade', 'aplicação', 'abordagens', 'PLN', 'estudo', 'conteúdos', 'comportamentos', 'gerados', 'meio', 'áreas', 'aplicação', 'abordagens', 'destacam-se', 'detecção', 'discurso', 'ódio', 'linguagem', 'ofensiva', 'detecção', 'ironia', 'sarcasmo', 'humor', 'detecção', 'notícias', 'falsas', 'análise', 'sentimento', 'outras', 'Ferreira', 'et', 'al.', '2017.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Tokenização"
      ],
      "metadata": {
        "id": "X7GqwK08DZUO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenizacao(text):\n",
        "  instruction = \"Você irá receber um texto em Português do Brasil e irá extrair todos os Tokens presentes no mesmo, retornando-os no formato de uma lista do Python e nada mais\"\n",
        "\n",
        "  print(\"Texto: {}\".format(text))\n",
        "  ai_response = openai.ChatCompletion.create(\n",
        "      model = 'gpt-3.5-turbo',\n",
        "      messages = [\n",
        "          {\"role\": \"system\", \"content\": instruction},\n",
        "          {\"role\": \"user\", \"content\": text}\n",
        "      ]\n",
        "  )\n",
        "\n",
        "  print(ai_response[\"choices\"][0][\"message\"][\"content\"])"
      ],
      "metadata": {
        "id": "7XfRO8RoDj20"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Uso da Técnica nos Capítulos"
      ],
      "metadata": {
        "id": "wHZHQ78TEVt0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Capítulo 3:\")\n",
        "tokenizacao(get_text_from_paragraph(response_cap_3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ii5e2DXKEIXE",
        "outputId": "bd7dc604-0eb3-49c9-a1c8-40b2f32c7171"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Capítulo 3:\n",
            "Texto: Até a metade de 2020, o português brasileiro (PB) possuía apenas algumas dezenas de horas de dados de fala públicos ou abertos para pesquisas acadêmicas, disponíveis para treinar modelos para os sistemas mais comuns, que são os reconhecedores automáticos de fala (em inglês, Automatic Speech Recognition ou ASR) e os sintetizadores de fala (em inglês, Text-to-Speech Synthesis ou TTS). Havia um grande contraste com a língua inglesa, cujos recursos eram maiores tanto em número de horas quanto em número de locutores e, assim, mais adequados à aplicação de métodos de aprendizado profundo de máquina, chamados de deep learning, em inglês.\n",
            "['Até', 'a', 'metade', 'de', '2020', ',', 'o', 'português', 'brasileiro', '(', 'PB', ')', 'possuía', 'apenas', 'algumas', 'dezenas', 'de', 'horas', 'de', 'dados', 'de', 'fala', 'públicos', 'ou', 'abertos', 'para', 'pesquisas', 'acadêmicas', ',', 'disponíveis', 'para', 'treinar', 'modelos', 'para', 'os', 'sistemas', 'mais', 'comuns', ',', 'que', 'são', 'os', 'reconhecedores', 'automáticos', 'de', 'fala', '(', 'em', 'inglês', ',', 'Automatic', 'Speech', 'Recognition', 'ou', 'ASR', ')', 'e', 'os', 'sintetizadores', 'de', 'fala', '(', 'em', 'inglês', ',', 'Text-to-Speech', 'Synthesis', 'ou', 'TTS', ')', '.', 'Havia', 'um', 'grande', 'contraste', 'com', 'a', 'língua', 'inglesa', ',', 'cujos', 'recursos', 'eram', 'maiores', 'tanto', 'em', 'número', 'de', 'horas', 'quanto', 'em', 'número', 'de', 'locutores', 'e', ',', 'assim', ',', 'mais', 'adequados', 'à', 'aplicação', 'de', 'métodos', 'de', 'aprendizado', 'profundo', 'de', 'máquina', ',', 'chamados', 'de', 'deep', 'learning', ',', 'em', 'inglês', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nCapítulo 23:\")\n",
        "tokenizacao(get_text_from_paragraph(response_cap_23))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GhS6aiTMEO9F",
        "outputId": "f656aa36-0e68-479a-fc4d-4388859a03ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Capítulo 23:\n",
            "Texto: Tal como diversas outras tarefas de aplicação de abordagens de PLN, apesar dos esforços recentes, a detecção de discurso de ódio em português fica muito atrás do inglês (Jahan; Oussalah, 2023). A detecção de discursos de ódio em língua portuguesa é, sem dúvida, uma área promissora de pesquisa no campo do PLN. Redes sociais são um terreno fértil para a disseminação de discursos de ódio em qualquer idioma. Dada a popularidade da língua portuguesa nas redes sociais, tem-se aqui uma área muito fértil para o desenvolvimento de estudos de aplicações de abordagens e PLN. A detecção de discursos de ódio em português envolve desafios únicos, como a diversidade linguística, o uso de gírias e expressões regionais, além das particularidades culturais. Isso torna a pesquisa nessa área empolgante e relevante, não apenas do ponto de vista técnico, mas também do ponto de vista social e ético.\n",
            "['Tal', 'como', 'diversas', 'outras', 'tarefas', 'de', 'aplicação', 'de', 'abordagens', 'de', 'PLN', ',', 'apesar', 'dos', 'esforços', 'recentes', ',', 'a', 'detecção', 'de', 'discurso', 'de', 'ódio', 'em', 'português', 'fica', 'muito', 'atrás', 'do', 'ing...e', 'estudos', 'de', 'aplicações', 'de', 'abordagens', 'e', 'PLN', '.', 'A', 'detecção', 'de', 'discursos', 'de', 'ódio', 'em', 'português', 'envolve', 'desafios', 'únicos', ',', 'como', 'a', 'diversidade', 'linguística', ',', 'o', 'uso', 'de', 'gírias']\n"
          ]
        }
      ]
    }
  ]
}